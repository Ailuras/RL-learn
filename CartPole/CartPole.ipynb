{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from myenv import MyEnv\n",
    "import math\n",
    "\n",
    "class CartPoleSolver():\n",
    "    \n",
    "    def __init__(self, gamma=1.0, epsilon=1.0, alpha=0.01, episodes=10, batch_size=1000, interval_num=6):\n",
    "        self.env = MyEnv()\n",
    "        self.gamma = gamma # 折扣因子\n",
    "        self.epsilon = epsilon # 贪婪策略参数\n",
    "        self.alpha = alpha # 学习率\n",
    "        self.episodes = episodes # 决策序列长度\n",
    "        self.batch_size = batch_size # 训练次数\n",
    "        self.interval_num = interval_num # 连续变量转离散变量分为几段\n",
    "\n",
    "        self.pa_bin = np.linspace(-math.pi, math.pi, interval_num+1)[1: -1]\n",
    "        self.pv_bin = np.linspace(-math.pi*15, math.pi*15, interval_num+1)[1: -1]\n",
    "\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(interval_num**4, 3))\n",
    "        \n",
    "    def get_state_index(self, observation):\n",
    "        pole_angle, pole_v = observation\n",
    "        \n",
    "        state_index = 0\n",
    "        state_index += np.digitize(pole_angle, bins = self.pa_bin) * self.interval_num\n",
    "        state_index += np.digitize(pole_v, bins = self.pv_bin)\n",
    "        \n",
    "        return state_index\n",
    "    \n",
    "    def update_Q_table(self, observation, action, reward, next_observation):        \n",
    "        state_index = self.get_state_index(observation)\n",
    "        next_state_index = self.get_state_index(next_observation)\n",
    "        \n",
    "        maxQ = max(self.q_table[next_state_index][:])\n",
    "        self.q_table[state_index, action] = self.q_table[state_index, action] + self.alpha * (reward + self.gamma*maxQ - self.q_table[state_index, action])\n",
    "        \n",
    "    def decide_action(self, observation, episode = 0):\n",
    "        \n",
    "        state = self.get_state_index(observation)\n",
    "        # epsilon = 0.5 * (1 / (episode + 1))\n",
    "        if self.epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.q_table[state][:])\n",
    "        else:\n",
    "            action = np.random.choice(3)\n",
    "            \n",
    "        return action\n",
    "    def run(self):\n",
    "        observation = self.env.reset()\n",
    "        for t in range(1000):\n",
    "            self.env.render()\n",
    "            # print(observation)\n",
    "            action = self.decide_action(observation)\n",
    "            next_observation, reward, _, _ = self.env.step(action)\n",
    "            self.update_Q_table(observation, action, reward, next_observation)\n",
    "            observation = next_observation\n",
    "    \n",
    "    def solve(self):\n",
    "        for _ in range(self.episodes):\n",
    "            self.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CartPoleSolver()\n",
    "a.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab4d63602da93bdf91a2bfeaf05c64f621c29aa4059f58758cfb44b9b483284c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
