{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Mod\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "\n",
    "class MyEnv(gym.Env[np.ndarray, Union[int, np.ndarray]]):\n",
    "    \"\"\"\n",
    "    新的环境状态仅包含角度[-pi, pi]和角速度[-15pi, 15pi]\n",
    "    动作包含-3v, 0, 3v\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.g = 9.81\n",
    "        self.m = 0.055\n",
    "        self.l = 0.042\n",
    "        self.J = 1.91e-4\n",
    "        self.b = 3e-6\n",
    "        self.K = 0.0536\n",
    "        self.R = 9.5\n",
    "        self.T_s = 0.005\n",
    "        self.umap = {0:-3, 1:0, 2:3}\n",
    "        self.R_rew = 1\n",
    "        self.Q_rew1 = 5\n",
    "        self.Q_rew2 = 0.1\n",
    "\n",
    "        self.a_threshold = 15 * math.pi\n",
    "        high = np.array(\n",
    "            [\n",
    "                math.pi,\n",
    "                self.a_threshold * 2,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "    def step(self, action, method):\n",
    "        theta, theta_v = self.state\n",
    "        u = self.umap[action]\n",
    "        \n",
    "        temp = 1/self.J * (self.m*self.g*self.l*math.sin(theta) - self.b*theta_v - self.K**2/self.R*theta_v + self.K/self.R*u)\n",
    "        theta_new = theta + self.T_s*theta_v\n",
    "        theta_v_new = theta_v + self.T_s*temp\n",
    "        if theta_new > math.pi:\n",
    "            theta_new -= math.pi*2\n",
    "        elif theta_new < -math.pi:\n",
    "            theta_new += math.pi*2\n",
    "        self.state = (theta_new, theta_v_new)\n",
    "        done = False\n",
    "        if method == 0:\n",
    "            reward = -self.Q_rew1*theta_new**2 - self.Q_rew2*theta_v_new**2 - self.R_rew*u**2\n",
    "        else:\n",
    "            theta_unit = math.pi*2/method\n",
    "            theta_v_unit = math.pi*30/method\n",
    "            theta_r = (int((theta_new+math.pi)/theta_unit) + 1/2) * theta_unit - math.pi\n",
    "            theta_v_r = (int((theta_v_new+math.pi*15)/theta_v_unit) + 1/2) * theta_v_unit - math.pi*15\n",
    "            reward = -self.Q_rew1*theta_r**2 - self.Q_rew2*theta_v_r**2 - self.R_rew*u**2\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        return_info: bool = False,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = (math.pi, 0)\n",
    "        # self.state = (0, 0)\n",
    "        if not return_info:\n",
    "            return np.array(self.state, dtype=np.float32)\n",
    "        else:\n",
    "            return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def render(self):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = 4.8\n",
    "        scale = screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.surf = pygame.Surface((screen_width, screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = screen_width / 2.0\n",
    "        carty = 150\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[0])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, screen_width, carty, (0, 0, 0))\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        \n",
    "        pygame.event.pump()\n",
    "        self.clock.tick(50)\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from myenv import MyEnv\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CartPoleSolver():\n",
    "    \n",
    "    def __init__(self, gamma=0.98, epsilon=0.99, alpha=0.25, episodes=1000, batch_size=20000, interval_num=100, error=1e-2):\n",
    "        self.env = MyEnv()\n",
    "        self.gamma = gamma # 折扣因子\n",
    "        self.epsilon = epsilon # 贪婪策略参数\n",
    "        self.alpha = alpha # 学习率\n",
    "        self.episodes = episodes # 决策序列长度\n",
    "        self.batch_size = batch_size # 训练次数\n",
    "        self.interval_num = interval_num # 连续变量转离散变量分为几段\n",
    "        self.error = error\n",
    "        self.error_table = []\n",
    "\n",
    "        self.pa_bin = np.linspace(-math.pi, math.pi, interval_num+1)[1: -1]\n",
    "        self.pv_bin = np.linspace(-math.pi*15, math.pi*15, interval_num+1)[1: -1]\n",
    "\n",
    "        # self.q_table = np.random.uniform(low=0, high=1, size=(interval_num**2, 3))\n",
    "        self.q_table = np.zeros((interval_num**2, 3), dtype= np.float64)\n",
    "        self.trail = np.zeros((interval_num**2, 3), dtype= np.float64)\n",
    "        \n",
    "    def get_state_index(self, observation):\n",
    "        pole_angle, pole_v = observation\n",
    "        \n",
    "        state_index = 0\n",
    "        state_index += np.digitize(pole_angle, bins = self.pa_bin) * self.interval_num\n",
    "        state_index += np.digitize(pole_v, bins = self.pv_bin)\n",
    "        \n",
    "        return state_index\n",
    "    \n",
    "    def update_Q_table(self, observation, action, reward, next_observation):        \n",
    "        state_index = self.get_state_index(observation)\n",
    "        next_state_index = self.get_state_index(next_observation)\n",
    "        # if self.trail[state_index, action] == 1:\n",
    "        #     return\n",
    "        # self.trail[state_index, action] = 1\n",
    "        max_next = max(self.q_table[next_state_index][:])\n",
    "        q_target = reward + self.gamma * max_next\n",
    "        self.q_table[state_index, action] = self.q_table[state_index, action] + self.alpha * (q_target - self.q_table[state_index, action])\n",
    "        \n",
    "    def decide_action(self, observation, epsilon):\n",
    "        \n",
    "        state = self.get_state_index(observation)\n",
    "        \n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.q_table[state][:])\n",
    "        else:\n",
    "            action = np.random.choice(3)\n",
    "            \n",
    "        return action\n",
    "    def run(self, epsilon=0, quiet=True):\n",
    "        # self.trail[self.trail > 0] = 0\n",
    "        observation = self.env.reset()\n",
    "        if not quiet:\n",
    "            for t in range(1000):\n",
    "                self.env.render()\n",
    "                action = self.decide_action(observation, epsilon)\n",
    "                next_observation, reward, _, _ = self.env.step(action, 0)\n",
    "                observation = next_observation\n",
    "            return\n",
    "        for t in range(self.batch_size):\n",
    "                # print(observation)\n",
    "            # action = self.decide_action(observation, self.epsilon)\n",
    "            action = self.decide_action(observation, epsilon)\n",
    "            next_observation, reward, _, _ = self.env.step(action, 0)\n",
    "            self.update_Q_table(observation, action, reward, next_observation)\n",
    "            observation = next_observation\n",
    "    \n",
    "    def compute_error(self, a, b):\n",
    "        return np.linalg.norm(np.mat(a)-np.mat(b))\n",
    "        # error = a-b\n",
    "        # return (error*error).max()\n",
    "    \n",
    "    def solve(self):\n",
    "        epsilon = self.epsilon\n",
    "        self.index = 0\n",
    "        for i in tqdm(range(self.episodes)):\n",
    "        # for i in range(self.episodes):\n",
    "            self.index += 1\n",
    "            q_table = self.q_table.copy()\n",
    "            # epsilon = self.epsilon/self.index\n",
    "            epsilon = epsilon * self.epsilon\n",
    "            self.run(epsilon)\n",
    "            error = self.compute_error(q_table, self.q_table)\n",
    "            self.error_table.append(error)\n",
    "            if error < self.error:\n",
    "                print('经过%d次迭代后收敛'%self.index)\n",
    "                break\n",
    "            # else:\n",
    "                # print('第%d次迭代，误差为%f，更新了%d个Q值'%(self.index, error,np.sum(self.trail)))\n",
    "                # print('第%d次迭代，误差为%f'%(self.index, error))\n",
    "        if self.index == self.episodes:\n",
    "            print('到达最大迭代次数，此时变化值为%f'%error)\n",
    "        \n",
    "    def get_Q_table(self):\n",
    "        for i in range(3):\n",
    "            print('action: ', i)\n",
    "            for j in range(self.interval_num**2):\n",
    "                a = int(j/self.interval_num)\n",
    "                b = j%self.interval_num\n",
    "                print('angel: ', a, ', angel_v: ', b)\n",
    "                print(self.q_table[j, i])\n",
    "\n",
    "    def plot_error(self):\n",
    "        x = range(self.index)\n",
    "        y = self.error_table\n",
    "        plt.title(\"Diff vs. Iteration Plot\")\n",
    "        plt.plot(x, y, label=\"Train_Loss_list\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"diff\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_Q_table(self):\n",
    "        def f(x, y, a):\n",
    "            index = self.get_state_index(np.array((x, y), dtype=np.float32))\n",
    "            return self.q_table[index, a]\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 6), facecolor='w')\n",
    "        # x = np.arange(-math.pi, math.pi, self.interval_num)\n",
    "        # y = np.arange(-15*math.pi, 15*math.pi, self.interval_num)\n",
    "        \n",
    "        x = np.linspace(-math.pi, math.pi, self.interval_num+1)[: -1]\n",
    "        y = np.linspace(-15*math.pi, 15*math.pi, self.interval_num+1)[: -1]\n",
    "        \n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Z1 = self.q_table[:, 0].reshape(self.interval_num, -1)\n",
    "        # Z2 = self.q_table[:, 1].reshape(self.interval_num, -1)\n",
    "        # Z3 = self.q_table[:, 2].reshape(self.interval_num, -1)\n",
    "        \n",
    "        Z1 = f(X, Y, 0)\n",
    "        Z2 = f(X, Y, 1)\n",
    "        Z3 = f(X, Y, 2)\n",
    "        # Z1 = np.arange(self.interval_num*self.interval_num).reshape(self.interval_num, -1)\n",
    "        # Z2 = np.arange(self.interval_num*self.interval_num, -1).reshape(self.interval_num, -1)\n",
    "        \n",
    "        ax = fig.add_subplot(131, projection='3d')\n",
    "        plt.title(\"Q table of action -15\")\n",
    "        ax.set_xlabel('theta[rad]')\n",
    "        ax.set_ylabel('theta_v[rad/s]')\n",
    "        ax.set_zlabel('Q_value')\n",
    "        ax.plot_surface(X, Y, Z1, cmap='rainbow')\n",
    "        # ax.contour(X, Y, Z1, zdim='z', offset=0, cmap='rainbow')\n",
    "        \n",
    "        ax = fig.add_subplot(132, projection='3d')\n",
    "        plt.title(\"Q table of action 0\")\n",
    "        ax.set_xlabel('theta[rad]')\n",
    "        ax.set_ylabel('theta_v[rad/s]')\n",
    "        ax.set_zlabel('Q_value')\n",
    "        ax.plot_surface(X, Y, Z2, cmap='rainbow')\n",
    "        # ax.contour(X, Y, Z2, zdim='z', offset=0, cmap='rainbow')\n",
    "        \n",
    "        ax = fig.add_subplot(133, projection='3d')\n",
    "        plt.title(\"Q table of action 15\")\n",
    "        ax.set_xlabel('theta[rad]')\n",
    "        ax.set_ylabel('theta_v[rad/s]')\n",
    "        ax.set_zlabel('Q_value')\n",
    "        ax.plot_surface(X, Y, Z3, cmap='rainbow')\n",
    "        # ax.contour(X, Y, Z3, zdim='z', offset=0, cmap='rainbow')\n",
    "        \n",
    "        plt.show()\n",
    "            \n",
    "    def plot_action_of_state(self):\n",
    "        # plt.title(\"action of state\")\n",
    "        q0 = self.q_table[:, 0]\n",
    "        q1 = self.q_table[:, 1]\n",
    "        q2 = self.q_table[:, 2]\n",
    "        action = self.q_table[:, 0]\n",
    "        \n",
    "        for i in range(self.interval_num*self.interval_num):\n",
    "            action[i] = np.argmax([q0[i], q1[i], q2[i]])\n",
    "        action = action.reshape(self.interval_num, -1)\n",
    "        print()\n",
    "        plt.matshow(action, cmap='rainbow')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "a = CartPoleSolver()\n",
    "a.solve()\n",
    "a.run(quiet=False)\n",
    "a.plot_error()\n",
    "a.plot_Q_table()\n",
    "a.plot_action_of_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from myenv import MyEnv\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CartPoleSolver():\n",
    "    \n",
    "    def __init__(self, gamma=0.98, epsilon=0.99, alpha=0.25, episodes=1000, batch_size=20000, interval_num=100, error=1e-2):\n",
    "        self.env = MyEnv()\n",
    "        self.gamma = gamma # 折扣因子\n",
    "        self.epsilon = epsilon # 贪婪策略参数\n",
    "        self.alpha = alpha # 学习率\n",
    "        self.episodes = episodes # 决策序列长度\n",
    "        self.batch_size = batch_size # 训练次数\n",
    "        self.interval_num = interval_num # 连续变量转离散变量分为几段\n",
    "        self.error = error\n",
    "        self.error_table = []\n",
    "\n",
    "        self.pa_bin = np.linspace(-math.pi, math.pi, interval_num+1)[1: -1]\n",
    "        self.pv_bin = np.linspace(-math.pi*15, math.pi*15, interval_num+1)[1: -1]\n",
    "\n",
    "        # self.q_table = np.random.uniform(low=0, high=1, size=(interval_num**2, 3))\n",
    "        self.q_table = np.zeros((interval_num**2, 3), dtype= np.float64)\n",
    "        self.trail = np.zeros((interval_num**2, 3), dtype= np.float64)\n",
    "        \n",
    "    def get_state_index(self, observation):\n",
    "        pole_angle, pole_v = observation\n",
    "        \n",
    "        state_index = 0\n",
    "        state_index += np.digitize(pole_angle, bins = self.pa_bin) * self.interval_num\n",
    "        state_index += np.digitize(pole_v, bins = self.pv_bin)\n",
    "        \n",
    "        return state_index\n",
    "    \n",
    "    def update_Q_table(self, observation, action, reward, next_observation, epsilon):        \n",
    "        state_index = self.get_state_index(observation)\n",
    "        next_state_index = self.get_state_index(next_observation)\n",
    "        # if self.trail[state_index, action] == 1:\n",
    "        #     return\n",
    "        # self.trail[state_index, action] = 1\n",
    "        # max_next = max(self.q_table[next_state_index][:])\n",
    "        # q_target = reward + self.gamma * max_next\n",
    "        a_next = self.decide_action(next_observation, epsilon)\n",
    "        q_target = reward + self.gamma * self.q_table[next_state_index, a_next]\n",
    "        self.q_table[state_index, action] = self.q_table[state_index, action] + self.alpha * (q_target - self.q_table[state_index, action])\n",
    "        \n",
    "    def decide_action(self, observation, epsilon):\n",
    "        \n",
    "        state = self.get_state_index(observation)\n",
    "        \n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.q_table[state][:])\n",
    "        else:\n",
    "            action = np.random.choice(3)\n",
    "            \n",
    "        return action\n",
    "    def run(self, epsilon=0, quiet=True):\n",
    "        # self.trail[self.trail > 0] = 0\n",
    "        observation = self.env.reset()\n",
    "        if not quiet:\n",
    "            for t in range(1000):\n",
    "                self.env.render()\n",
    "                action = self.decide_action(observation, epsilon)\n",
    "                next_observation, reward, _, _ = self.env.step(action, 0)\n",
    "                observation = next_observation\n",
    "            return\n",
    "        for t in range(self.batch_size):\n",
    "                # print(observation)\n",
    "            # action = self.decide_action(observation, self.epsilon)\n",
    "            action = self.decide_action(observation, epsilon)\n",
    "            next_observation, reward, _, _ = self.env.step(action, 0)\n",
    "            self.update_Q_table(observation, action, reward, next_observation, epsilon)\n",
    "            observation = next_observation\n",
    "    \n",
    "    def compute_error(self, a, b):\n",
    "        return np.linalg.norm(np.mat(a)-np.mat(b))\n",
    "        # error = a-b\n",
    "        # return (error*error).max()\n",
    "    \n",
    "    def solve(self):\n",
    "        epsilon = self.epsilon\n",
    "        self.index = 0\n",
    "        for i in tqdm(range(self.episodes)):\n",
    "        # for i in range(self.episodes):\n",
    "            self.index += 1\n",
    "            q_table = self.q_table.copy()\n",
    "            # epsilon = self.epsilon/self.index\n",
    "            epsilon = epsilon * self.epsilon\n",
    "            self.run(epsilon)\n",
    "            error = self.compute_error(q_table, self.q_table)\n",
    "            self.error_table.append(error)\n",
    "            if error < self.error:\n",
    "                print('经过%d次迭代后收敛'%self.index)\n",
    "                break\n",
    "            # else:\n",
    "                # print('第%d次迭代，误差为%f，更新了%d个Q值'%(self.index, error,np.sum(self.trail)))\n",
    "                # print('第%d次迭代，误差为%f'%(self.index, error))\n",
    "        if self.index == self.episodes:\n",
    "            print('到达最大迭代次数，此时变化值为%f'%error)\n",
    "        \n",
    "    def get_Q_table(self):\n",
    "        for i in range(3):\n",
    "            print('action: ', i)\n",
    "            for j in range(self.interval_num**2):\n",
    "                a = int(j/self.interval_num)\n",
    "                b = j%self.interval_num\n",
    "                print('angel: ', a, ', angel_v: ', b)\n",
    "                print(self.q_table[j, i])\n",
    "\n",
    "    def plot_error(self):\n",
    "        x = range(self.index)\n",
    "        y = self.error_table\n",
    "        plt.title(\"Diff vs. Iteration Plot\")\n",
    "        plt.plot(x, y, label=\"Train_Loss_list\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"diff\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_Q_table(self):\n",
    "        def f(x, y, a):\n",
    "            index = self.get_state_index(np.array((x, y), dtype=np.float32))\n",
    "            return self.q_table[index, a]\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 6), facecolor='w')\n",
    "        # x = np.arange(-math.pi, math.pi, self.interval_num)\n",
    "        # y = np.arange(-15*math.pi, 15*math.pi, self.interval_num)\n",
    "        \n",
    "        x = np.linspace(-math.pi, math.pi, self.interval_num+1)[: -1]\n",
    "        y = np.linspace(-15*math.pi, 15*math.pi, self.interval_num+1)[: -1]\n",
    "        \n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Z1 = self.q_table[:, 0].reshape(self.interval_num, -1)\n",
    "        # Z2 = self.q_table[:, 1].reshape(self.interval_num, -1)\n",
    "        # Z3 = self.q_table[:, 2].reshape(self.interval_num, -1)\n",
    "        \n",
    "        Z1 = f(X, Y, 0)\n",
    "        Z2 = f(X, Y, 1)\n",
    "        Z3 = f(X, Y, 2)\n",
    "        # Z1 = np.arange(self.interval_num*self.interval_num).reshape(self.interval_num, -1)\n",
    "        # Z2 = np.arange(self.interval_num*self.interval_num, -1).reshape(self.interval_num, -1)\n",
    "        \n",
    "        ax = fig.add_subplot(131, projection='3d')\n",
    "        plt.title(\"Q table of action -15\")\n",
    "        ax.set_xlabel('theta[rad]')\n",
    "        ax.set_ylabel('theta_v[rad/s]')\n",
    "        ax.set_zlabel('Q_value')\n",
    "        ax.plot_surface(X, Y, Z1, cmap='rainbow')\n",
    "        # ax.contour(X, Y, Z1, zdim='z', offset=0, cmap='rainbow')\n",
    "        \n",
    "        ax = fig.add_subplot(132, projection='3d')\n",
    "        plt.title(\"Q table of action 0\")\n",
    "        ax.set_xlabel('theta[rad]')\n",
    "        ax.set_ylabel('theta_v[rad/s]')\n",
    "        ax.set_zlabel('Q_value')\n",
    "        ax.plot_surface(X, Y, Z2, cmap='rainbow')\n",
    "        # ax.contour(X, Y, Z2, zdim='z', offset=0, cmap='rainbow')\n",
    "        \n",
    "        ax = fig.add_subplot(133, projection='3d')\n",
    "        plt.title(\"Q table of action 15\")\n",
    "        ax.set_xlabel('theta[rad]')\n",
    "        ax.set_ylabel('theta_v[rad/s]')\n",
    "        ax.set_zlabel('Q_value')\n",
    "        ax.plot_surface(X, Y, Z3, cmap='rainbow')\n",
    "        # ax.contour(X, Y, Z3, zdim='z', offset=0, cmap='rainbow')\n",
    "        \n",
    "        plt.show()\n",
    "            \n",
    "    def plot_action_of_state(self):\n",
    "        # plt.title(\"action of state\")\n",
    "        q0 = self.q_table[:, 0]\n",
    "        q1 = self.q_table[:, 1]\n",
    "        q2 = self.q_table[:, 2]\n",
    "        action = self.q_table[:, 0]\n",
    "        \n",
    "        for i in range(self.interval_num*self.interval_num):\n",
    "            action[i] = np.argmax([q0[i], q1[i], q2[i]])\n",
    "        action = action.reshape(self.interval_num, -1)\n",
    "        print()\n",
    "        plt.matshow(action, cmap='rainbow')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "a = CartPoleSolver()\n",
    "a.solve()\n",
    "a.run(quiet=False)\n",
    "a.plot_error()\n",
    "a.plot_Q_table()\n",
    "a.plot_action_of_state()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
